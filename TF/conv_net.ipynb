{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "\n",
    "# Convert classes to indicator vectors\n",
    "def one_hot(values,n_values=10):\n",
    "    n_v = np.maximum(n_values,np.max(values) + 1)\n",
    "    oh=np.eye(n_v)[values]\n",
    "    return oh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Mnist data and split into train validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "\n",
    "    labels=np.float32(np.load('/project/cmsc25025/mnist/MNIST_labels.npy'))\n",
    "    data=np.float64(np.load('/project/cmsc25025/mnist/MNIST.npy'))\n",
    "    print(data.shape)\n",
    "    data=np.float32(data)/255.\n",
    "    train_dat=data[0:50000]\n",
    "    train_labels=one_hot(np.int32(labels[0:50000]))\n",
    "    val_dat=data[50000:60000]\n",
    "    val_labels=one_hot(np.int32(labels[50000:60000]))\n",
    "    test_dat=data[60000:70000]\n",
    "    test_labels=one_hot(np.int32(labels[60000:70000]))\n",
    "    \n",
    "    return (train_dat, train_labels), (val_dat, val_labels), (test_dat, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "HH=get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input,filter_size=[3,3],num_features=[1]):\n",
    "\n",
    "    # Get number of input features from input and add to shape of new layer\n",
    "    shape=filter_size+[input.get_shape().as_list()[-1],num_features]\n",
    "    W = tf.get_variable('W',shape=shape) # Default initialization is Glorot (the one explained in the slides)\n",
    "    #b = tf.get_variable('b',shape=[num_features],initializer=tf.zeros_initializer) \n",
    "    conv = tf.nn.conv2d(input, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    return(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layer(input,num_features):\n",
    "    # Make sure input is flattened.\n",
    "    flat_dim=np.int32(np.array(input.get_shape().as_list())[1:].prod())\n",
    "    input_flattened = tf.reshape(input, shape=[-1,flat_dim])\n",
    "    shape=[flat_dim,num_features]\n",
    "    W_fc = tf.get_variable('W',shape=shape) \n",
    "    #b_fc = tf.get_variable('b',shape=[num_features],initializer=tf.zeros_initializer)\n",
    "    fc = tf.matmul(input_flattened, W_fc) # + b_fc\n",
    "    return(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    " \n",
    "def MaxPoolingandMask(inputs, pool_size, strides,\n",
    "                          padding='SAME'):\n",
    "\n",
    "        pooled = tf.nn.max_pool(inputs, ksize=pool_size, strides=strides, padding=padding)\n",
    "        upsampled = UpSampling2D(size=strides[1:3])(pooled)\n",
    "        indexMask = K.tf.equal(inputs, upsampled)\n",
    "        assert indexMask.get_shape().as_list() == inputs.get_shape().as_list()\n",
    "        return pooled,indexMask\n",
    "     \n",
    "#def get_output_shape_for(self, input_shape):\n",
    "#        return input_shape\n",
    " \n",
    " \n",
    "def unpooling(x,mask,strides):\n",
    "    '''\n",
    "    do unpooling with indices, move this to separate layer if it works\n",
    "    1. do naive upsampling (repeat elements)\n",
    "    2. keep only values in mask (stored indices) and set the rest to zeros\n",
    "    '''\n",
    "    on_success = UpSampling2D(size=strides)(x)\n",
    "    on_fail = K.zeros_like(on_success)\n",
    "    return K.tf.where(mask, on_success, on_fail)\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_small_network():\n",
    "\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "            # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "            conv1 = conv_layer(x_image, filter_size=[3, 3],num_features=16)\n",
    "    with tf.variable_scope(\"conv1K\"):\n",
    "            conv1K = tf.clip_by_value(conv1,-1.,1.)\n",
    "    with tf.variable_scope(\"pool1\"):\n",
    "            pool1, mask1 = MaxPoolingandMask(conv1, pool_size=[1,3,3,1], strides=[1,2,2,1])\n",
    "    print('pool1','mask1',pool1.shape,mask1.shape)\n",
    "    with tf.variable_scope(\"fc\"):\n",
    "            fc = fully_connected_layer(pool1, num_features=10)\n",
    "    with tf.variable_scope('cross_entropy_loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=fc),name=\"LOSS\")\n",
    "\n",
    "#     with tf.variable_scope(\"conv1_a\"):\n",
    "#             # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "#             conv1_a = conv_layer(x_image, filter_size=[3, 3],num_features=4)\n",
    "#     with tf.variable_scope(\"fc_a\"):\n",
    "#             fc_a = fully_connected_layer(conv1_a, num_features=10)\n",
    "#     with tf.variable_scope('cross_entropy_loss_a'):\n",
    "#         cross_entropy_a = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=fc_a),name=\"LOSSa\")\n",
    "\n",
    "    # Accuracy computation\n",
    "    with tf.variable_scope('helpers'):\n",
    "        correct_prediction = tf.equal(tf.argmax(fc, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"ACC\")\n",
    "    # We return the final functions (they contain all the information about the graph of the network)\n",
    "    return cross_entropy, accuracy, conv1, conv1K, fc, pool1, mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loss and accuracy on a data set with output from final layer fc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get loss and accuracy from only one run of the feature extraction network\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def get_stats(data,labels):\n",
    "    t1=time.time()\n",
    "    lo=0.\n",
    "    acc=0.\n",
    "    delta=1000\n",
    "    rr=np.arange(0,data.shape[0],delta)\n",
    "    for i in rr:\n",
    "        fc_out=fc.eval(feed_dict={x: data[i:i+delta], y_:labels[i:i+delta]})\n",
    "        log_sf=logsumexp(fc_out,axis=1).reshape((fc_out.shape[0],1))-fc_out\n",
    "        lo+=np.mean(np.sum(labels[i:i+delta]*log_sf, axis=1))\n",
    "        acc += np.mean(np.equal(np.argmax(fc_out, axis=1),np.argmax(labels[i:i+delta], axis=1)))\n",
    "    acc=acc/np.float32(len(rr))\n",
    "    lo=lo/np.float32(len(rr))\n",
    "    print('get stats time',time.time()-t1)\n",
    "    # We return the final functions (they contain all the information about the graph of the network)\n",
    "    return lo, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(): #(VS,cross_entropy,conv,fc):\n",
    "    #convx, fcx = sess.run([conv,fc],feed_dict={x: batch[0], y_: batch[1]})\n",
    "    # Get gradient of loss with respect to final output layer using tf gradient\n",
    "    # The rest will be explicit backprop\n",
    "    gradx=tf.gradients(cross_entropy,fc)\n",
    "    #gradx=sess.run(GRADx,feed_dict={x: batch[0], y_: batch[1]})\n",
    "    #print('gradx',gradx)\n",
    "    # Flatten convolutional output layer\n",
    "    poolf=tf.contrib.layers.flatten(pool)\n",
    "    # Gradient of weights of dense layer\n",
    "    gradfcW=tf.matmul(tf.transpose(poolf),gradx[0])\n",
    "    # Propagated error to conv layer.\n",
    "    gradfcx_pool=tf.matmul(gradx[0],tf.transpose(VS[1]))\n",
    "    # Reshape propagated error to spatial dims.\n",
    "    gradfcx_pool=tf.reshape(gradfcx_pool,[-1]+(pool.shape.as_list())[1:])\n",
    "    gradfcx=unpooling(gradfcx_pool,mask,[2,2])\n",
    "    #print('gradfcx',gradfcx.shape)\n",
    "    # Get gradient of conv layer W's\n",
    "    w_shape=VS[0].shape\n",
    "    strides=[1,1,1,1]\n",
    "    out_backprop=tf.reshape(gradfcx,[-1]+(conv.shape.as_list())[1:])\n",
    "    on_fail = K.zeros_like(out_backprop)\n",
    "    out_backpropF=K.tf.where(tf.equal(tf.abs(convK),1.),out_backprop,on_fail)\n",
    "    gradconvW=tf.nn.conv2d_backprop_filter(input=x_image,filter_sizes=w_shape,\\\n",
    "                                                     out_backprop=out_backpropF,\\\n",
    "                                                     strides=strides,\\\n",
    "                                                     padding='SAME')\n",
    "    #print('gradconvW',gradconvW)\n",
    "    #print('gradfcW',gradfcW)\n",
    "    return gradconvW, gradfcW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the iterations of one epoch\n",
    "def run_epoch(train,val,ii,batch_size,cross_entropy,dWs,step_size):\n",
    "        t1=time.time()\n",
    "        # Randomly shuffle the training data\n",
    "        np.random.shuffle(ii)\n",
    "        tr=train[0][ii]\n",
    "        y=train[1][ii]\n",
    "        lo=0.\n",
    "        acc=0.\n",
    "        #VS=tf.trainable_variables()\n",
    "        # Run disjoint batches on shuffled data\n",
    "        for j in np.arange(0,len(y),batch_size):\n",
    "            if (np.mod(j,5000)==0):\n",
    "                print('Batch',j/batch_size)\n",
    "            batch=(tr[j:j+batch_size],y[j:j+batch_size])\n",
    "            grad=sess.run(dWs,feed_dict={x: batch[0], y_: batch[1]})\n",
    "            \n",
    "            for gr, vs in zip(grad,VS):\n",
    "              sess.run(vs.assign(vs - step_size*gr))\n",
    "        print('Epoch time',time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data and run the training. Save the model and test at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_set):\n",
    "    if (data_set==\"cifar\"):\n",
    "        return(get_cifar())\n",
    "    elif (data_set==\"mnist\"):\n",
    "        return(get_mnist())\n",
    "    elif (data_set==\"mnist_transform\"):\n",
    "        return(get_mnist_trans())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "pool1 mask1 (?, 14, 14, 16) (?, 28, 28, 16)\n",
      "conv1/W:0 [3, 3, 1, 16] 0.11082865\n",
      "fc/W:0 [3136, 10] 0.025171598\n",
      "Batch 0.0\n",
      "Batch 50.0\n",
      "Batch 100.0\n",
      "Batch 150.0\n",
      "Batch 200.0\n",
      "Batch 250.0\n",
      "Batch 300.0\n",
      "Batch 350.0\n",
      "Batch 400.0\n",
      "Batch 450.0\n",
      "Epoch time 66.12982511520386\n",
      "get stats time 3.3394076824188232\n",
      "Epoch 0 Train loss, accuracy 0.4575508047032357 0.8913399999999999\n",
      "get stats time 0.8108248710632324\n",
      "EPoch 0 Validation loss, accuracy 0.4319435055494309 0.9037\n",
      "Batch 0.0\n",
      "Batch 50.0\n",
      "Batch 100.0\n",
      "Batch 150.0\n",
      "Batch 200.0\n",
      "Batch 250.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e4638677bf6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Run epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdWs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-1f95dcc05337>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train, val, ii, batch_size, cross_entropy, dWs, step_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m               \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1307\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda2/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "\n",
    "import time\n",
    "batch_size=100\n",
    "step_size=.1\n",
    "num_epochs=10\n",
    "num_train=50000\n",
    "minimizer=\"SGD\"\n",
    "data_set=\"mnist\"\n",
    "model_name=\"model\"\n",
    "keep_prob=.5\n",
    "dim=28\n",
    "nchannels=1\n",
    "if (data_set==\"cifar\"):\n",
    "    dim=32\n",
    "    nchannels=3\n",
    "\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, dim*dim*nchannels],name=\"x\")\n",
    "x_image = tf.reshape(x, [-1, dim, dim, nchannels])\n",
    "# Dimensions of x_image: [Batch size, Column size, Row size, Number of incoming channels]\n",
    "# The number of incoming channels, for example, will be 3 if the image is color: RGB (red, green, blue)\n",
    "# We will slide filter over this 2d picture with conv2d function.\n",
    "y_ = tf.placeholder(tf.float32, shape=[None,10],name=\"y\")\n",
    "# Allows you to control the time step during the iterations\n",
    "lr_ = tf.placeholder(tf.float32, shape=[],name=\"learning_rate\")\n",
    "keep_prob_=tf.placeholder(tf.float32, shape=[],name=\"keep_prob\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train,val,test=get_data(data_set=data_set)\n",
    "    \n",
    "    # Create the network architecture with the above placeholdes as the inputs.\n",
    "    cross_entropy, accuracy, conv, convK, fc, pool, mask =create_small_network()\n",
    "    #train_step = tf.train.GradientDescentOptimizer(learning_rate=lr_).minimize(cross_entropy)\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Show trainable variables\n",
    "    VS=tf.trainable_variables()\n",
    "    for v in VS:\n",
    "        print(v.name,v.get_shape().as_list(),np.std(v.eval()))\n",
    "    dWs=back_prop() #(VS,cross_entropy,conv,fc)\n",
    "    ii=np.arange(0,num_train,1) #len(train_data),1)\n",
    "\n",
    "    # Run epochs\n",
    "    for i in range(num_epochs):  # number of epochs\n",
    "        run_epoch(train,val,ii,batch_size,cross_entropy,dWs,step_size)\n",
    "        \n",
    "        if (np.mod(i,2)==0):\n",
    "            lo,ac = get_stats(train[0][0:num_train],train[1][0:num_train])\n",
    "            print('Epoch',i,'Train loss, accuracy',lo,ac)\n",
    "            vlo,vac = get_stats(val[0],val[1])\n",
    "            print('EPoch',i,'Validation loss, accuracy',vlo,vac)\n",
    "            # Test set accuracy\n",
    " \n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: test[0], y_:test[1]}))\n",
    "    \n",
    "    # Save model\n",
    "    #tf.add_to_collection(\"optimizer\", train_step)\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"tmp/\"+model_name)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload the model that was saved and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading an existing model.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "batch_size=500\n",
    "step_size=.001\n",
    "num_epochs=4\n",
    "num_train=10000\n",
    "data_set=\"cifar\"\n",
    "model_name=\"model\"\n",
    "Train=True\n",
    "dim=28\n",
    "nchannels=1\n",
    "if (data_set==\"cifar\"):\n",
    "    dim=32\n",
    "    nchannels=3\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # Get data\n",
    "    train, val, test=get_data(data_set=data_set)\n",
    "    # Load model info\n",
    "    saver = tf.train.import_meta_graph('tmp/'+model_name+'.meta')\n",
    "    saver.restore(sess,'tmp/'+model_name) \n",
    "    graph = tf.get_default_graph()\n",
    "    # Setup the placeholders from the stored model.\n",
    "    x = graph.get_tensor_by_name('x:0')\n",
    "    y_= graph.get_tensor_by_name('y:0')\n",
    "    lr_ = graph.get_tensor_by_name('learning_rate:0')\n",
    "    keep_prob_ = graph.get_tensor_by_name('keep_prob:0')\n",
    "    accuracy=graph.get_tensor_by_name('helpers/ACC:0')\n",
    "    cross_entropy=graph.get_tensor_by_name('cross_entropy_loss/LOSS:0')\n",
    "    fc=graph.get_tensor_by_name('OUT:0')\n",
    "    # Get the minimization operation from the stored model\n",
    "    if (Train):\n",
    "        train_step_new = tf.get_collection(\"optimizer\")[0]\n",
    "        # Confirm training accuracy of current model before additional training\n",
    "        acc=accuracy.eval(feed_dict={x: train[0][0:num_train], y_:train[1][0:num_train]})\n",
    "        print('train acc',acc)\n",
    "\n",
    "        ii=np.arange(0,num_train,1) \n",
    "        for i in range(num_epochs):  # Run epochs\n",
    "            run_epoch(train,val,ii,batch_size,train_step_new)\n",
    "            if (np.mod(i,2)==0):\n",
    "                lo,ac = get_stats(train[0][0:num_train],train[1][0:num_train])\n",
    "                print('Epoch',i,'Train loss, accuracy',lo,ac)\n",
    "                vlo,vac = get_stats(val[0],val[1])\n",
    "                print('EPoch',i,'Validation loss, accuracy',vlo,vac)\n",
    "    # Test set accuracy\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: test[0], y_:test[1]}))\n",
    "    \n",
    "    tf.add_to_collection(\"optimizer\", train_step)\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"tmp/\"+model_name)\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
