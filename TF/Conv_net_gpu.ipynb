{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU_cuda_9.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input,filter_size=[3,3],num_features=[1],prob=[1.,-1.]):\n",
    "    \n",
    "    # Get number of input features from input and add to shape of new layer\n",
    "    shape=filter_size+[input.get_shape().as_list()[-1],num_features]\n",
    "    shapeR=shape\n",
    "    if (prob[1]==-1.):\n",
    "        shapeR=[1,1]\n",
    "    R = tf.get_variable('R',shape=shapeR)\n",
    "    W = tf.get_variable('W',shape=shape) # Default initialization is Glorot (the one explained in the slides)\n",
    "    \n",
    "    #b = tf.get_variable('b',shape=[num_features],initializer=tf.zeros_initializer) \n",
    "    conv = tf.nn.conv2d(input, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.clip_by_value(conv,-1.,1.)\n",
    "    \n",
    "    return(conv)\n",
    "\n",
    "def grad_conv_layer(below, back_propped, current, W, R):\n",
    "    w_shape=W.shape\n",
    "    strides=[1,1,1,1]\n",
    "    back_prop_shape=[-1]+(current.shape.as_list())[1:]\n",
    "    out_backprop=tf.reshape(back_propped,back_prop_shape)\n",
    "    on_zero = K.zeros_like(out_backprop)\n",
    "    out_backpropF=K.tf.where(tf.equal(tf.abs(current),1.),on_zero,out_backprop)\n",
    "    gradconvW=tf.nn.conv2d_backprop_filter(input=below,filter_sizes=w_shape,\\\n",
    "                                                     out_backprop=out_backpropF,\\\n",
    "                                                     strides=strides,\\\n",
    "                                           padding='SAME')\n",
    "    input_shape=[batch_size]+(below.shape.as_list())[1:]\n",
    "    \n",
    "    filter=W\n",
    "    if (len(R.shape.as_list())==4):\n",
    "        filter=R\n",
    "    gradconvx=tf.nn.conv2d_backprop_input(input_sizes=input_shape,filter=filter,out_backprop=out_backpropF,strides=strides,padding='SAME')\n",
    "    \n",
    "    return gradconvW, gradconvx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layer(input,num_features,prob=[1.,-1.]):\n",
    "    # Make sure input is flattened.\n",
    "    flat_dim=np.int32(np.array(input.get_shape().as_list())[1:].prod())\n",
    "    input_flattened = tf.reshape(input, shape=[-1,flat_dim])\n",
    "    shape=[flat_dim,num_features]\n",
    "    shapeR=shape\n",
    "    if (prob[1]==-1.):\n",
    "        shapeR=[1]\n",
    "    R_fc = tf.get_variable('R',shape=shapeR)\n",
    "    W_fc = tf.get_variable('W',shape=shape)\n",
    "\n",
    "    #b_fc = tf.get_variable('b',shape=[num_features],initializer=tf.zeros_initializer)\n",
    "    fc = tf.matmul(input_flattened, W_fc) # + b_fc\n",
    "    return(fc)\n",
    "\n",
    "def grad_fully_connected(below, back_propped, W, R):\n",
    "    \n",
    "    belowf=tf.contrib.layers.flatten(below)\n",
    "    # Gradient of weights of dense layer\n",
    "    gradfcW=tf.matmul(tf.transpose(belowf),back_propped)\n",
    "    # Propagated error to conv layer.\n",
    "    filter=W\n",
    "    if (len(R.shape.as_list())==2):\n",
    "        filter=R\n",
    "    gradfcx=tf.matmul(back_propped,tf.transpose(filter))\n",
    "    \n",
    "    return gradfcW, gradfcx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    " \n",
    "def MaxPoolingandMask(inputs, pool_size, strides,\n",
    "                          padding='SAME'):\n",
    "\n",
    "        pooled = tf.nn.max_pool(inputs, ksize=pool_size, strides=strides, padding=padding)\n",
    "        upsampled = UpSampling2D(size=strides[1:3])(pooled)\n",
    "        indexMask = K.tf.equal(inputs, upsampled)\n",
    "        assert indexMask.get_shape().as_list() == inputs.get_shape().as_list()\n",
    "        return pooled,indexMask\n",
    "     \n",
    "#def get_output_shape_for(self, input_shape):\n",
    "#        return input_shape\n",
    " \n",
    " \n",
    "def unpooling(x,mask,strides):\n",
    "    '''\n",
    "    do unpooling with indices, move this to separate layer if it works\n",
    "    1. do naive upsampling (repeat elements)\n",
    "    2. keep only values in mask (stored indices) and set the rest to zeros\n",
    "    '''\n",
    "    on_success = UpSampling2D(size=strides)(x)\n",
    "    on_fail = K.zeros_like(on_success)\n",
    "    return K.tf.where(mask, on_success, on_fail)\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "def grad_pool(back_propped,pool,mask,pool_size):\n",
    "        gradx_pool=tf.reshape(back_propped,[-1]+(pool.shape.as_list())[1:])\n",
    "    #gradfcx=tf.reshape(gradfcx_pool,[-1]+(conv.shape.as_list())[1:])\n",
    "        gradx=unpooling(gradx_pool,mask,pool_size)\n",
    "        return gradx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sibling(l,parent):\n",
    "      \n",
    "        for ly in PARS['layers']:\n",
    "            if ('parent' in ly):\n",
    "                q=ly['parent']\n",
    "\n",
    "                if (ly is not l and type(q)==str and q in parent):\n",
    "                    return q\n",
    "        return None  \n",
    "\n",
    "def create_network(PARS):\n",
    "    TS=[]\n",
    "    ln=len(PARS['layers'])\n",
    "    sibs={}\n",
    "    for i,l in enumerate(PARS['layers']):\n",
    "        parent=None\n",
    "        prob=[1.,-1.]\n",
    "        if ('force_global_prob' in PARS):\n",
    "            prob=list(PARS['force_global_prob'])\n",
    "        # Last output layer is fully connected to last hidden layer\n",
    "        if (i==ln-1):\n",
    "            prob[0]=1.\n",
    "        if ('parent' in l):\n",
    "            if ('input' in l['parent']):\n",
    "                parent=x\n",
    "            else:\n",
    "                # Get list of parents\n",
    "                if (type(l['parent'])==list):\n",
    "                    parent=[] \n",
    "                    for s in l['parent']:\n",
    "                        for ts in TS:\n",
    "                            if s in ts.name and not 'Equal' in ts.name:\n",
    "                                parent.append(ts)\n",
    "                # Get single parent\n",
    "                else:\n",
    "                    for ts in TS:\n",
    "                        if l['parent'] in ts.name and not 'Equal' in ts.name:\n",
    "                            parent=ts\n",
    "        if ('conv' in l['name']):\n",
    "            with tf.variable_scope(l['name']):\n",
    "                TS.append(conv_layer(parent, filter_size=list(l['filter_size']),num_features=l['num_filters'], prob=prob))\n",
    "        elif ('dens' in l['name']):\n",
    "            with tf.variable_scope(l['name']):\n",
    "                num_units=l['num_units']\n",
    "                if ('final' in l):\n",
    "                    num_units=n_classes\n",
    "                TS.append(fully_connected_layer(parent, num_features=num_units,prob=prob))\n",
    "        elif ('pool' in l['name']):\n",
    "            with tf.variable_scope(l['name']):\n",
    "                pool, mask = MaxPoolingandMask(parent, [1]+list(l['pool_size'])+[1],\\\n",
    "                                           strides=[1]+list(l['stride'])+[1])\n",
    "                TS.append(pool)\n",
    "                TS.append(mask)\n",
    "        elif ('drop' in l['name']):\n",
    "            with tf.variable_scope(l['name']):\n",
    "                U=tf.random_uniform([batch_size]+(parent.shape.as_list())[1:])<l['drop']\n",
    "                Z=tf.zeros_like(parent)\n",
    "                drop = K.tf.where(U,Z,parent)\n",
    "                TS.append(drop)\n",
    "        elif ('concatsum' in l['name']):\n",
    "            with tf.variable_scope(l['name']):\n",
    "                res_sum=tf.add(parent[0],parent[1])\n",
    "                TS.append(res_sum)\n",
    "            # This is a sum layer get its sibling\n",
    "                joint_parent=find_sibling(l,l['parent'])\n",
    "                if (joint_parent is not None):\n",
    "                    sibs[TS[-1].name]=joint_parent\n",
    "    \n",
    "    with tf.variable_scope('cross_entropy_loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=TS[-1]),name=\"LOSS\")\n",
    "    \n",
    "#     with tf.variable_scope('hinge_loss'):\n",
    "#         cor=tf.boolean_mask(TS[-1],y_)\n",
    "#         res=tf.boolean_mask(TS[-1],tf.subtract(tf.ones_like(y_),y_))\n",
    "#        tf.nn.relu(1.-cor)+PARS['dep_fac']*tf.reduce_sum(tf.nn.relu(1.+res),axis=1)/(n_classes-1)\n",
    "        \n",
    "    # Accuracy computation\n",
    "    with tf.variable_scope('helpers'):\n",
    "        correct_prediction = tf.equal(tf.argmax(TS[-1], 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"ACC\")\n",
    "    print('sibs',sibs)\n",
    "    return cross_entropy, accuracy, TS, sibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_only_non_zero(V,gra, step):\n",
    "    up=V-step*gra\n",
    "    up=K.tf.where(V==0,V,up)\n",
    "    assign_op = tf.assign(V,up)\n",
    "    return assign_op\n",
    "\n",
    "def back_prop(): #(VS,cross_entropy,conv,fc):\n",
    "    # Get gradient of loss with respect to final output layer using tf gradient\n",
    "    # The rest will be explicit backprop\n",
    "    \n",
    "    gradX=tf.gradients(cross_entropy,TS[0])\n",
    "    corr = tf.equal(tf.argmax(TS[0], 1), tf.argmax(y_, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(corr, tf.float32))\n",
    "\n",
    "    gradx=gradX[0]\n",
    "    lvs=len(VS)\n",
    "    lts=len(TS)\n",
    "    vs=0\n",
    "    ts=0\n",
    "    OPLIST=[]\n",
    "    grad_hold_var={}\n",
    "    parent=None\n",
    "    for ts in range(lts):\n",
    "        T=TS[ts]\n",
    "        if (ts<lts-1):\n",
    "                pre=TS[ts+1]\n",
    "                if ('Equal' in pre.name):\n",
    "                    pre=TS[ts+2]\n",
    "        else:\n",
    "            pre=x\n",
    "        # You have held a gradx from a higher up layer to be added to current one.\n",
    "        if (parent is not None and parent in T.name):\n",
    "            print('grad_hold',grad_hold_var[parent])\n",
    "            gradx=tf.add(gradx,grad_hold_var[parent])\n",
    "            parent=None\n",
    "        if ('conv' in T.name):  \n",
    "            gradconvW, gradx = grad_conv_layer(below=pre,back_propped=gradx,current=TS[ts],W=VS[vs], R=VS[vs+1])\n",
    "            assign_op_convW = update_only_non_zero(VS[vs],gradconvW,step_size)\n",
    "            #assign_op_convW=tf.assign(VS[vs],VS[vs]-step_size*gradconvW)\n",
    "            OPLIST.append(assign_op_convW)\n",
    "            if (len(VS[vs+1].shape.as_list())==4):\n",
    "                assign_op_convR=update_only_non_zero(VS[vs+1],gradconvW, Rstep_size)\n",
    "                #assign_op_convR=tf.assign(VS[vs+1],VS[vs+1]-Rstep_size*gradconvW)\n",
    "                OPLIST.append(assign_op_convR)\n",
    "            ts+=1\n",
    "            vs+=2\n",
    "        elif ('Equal' in T.name):\n",
    "            mask=TS[ts]\n",
    "            ts+=1\n",
    "        elif ('Max' in T.name):\n",
    "            gradx=grad_pool(gradx,TS[ts],mask,[2,2])  \n",
    "            ts+=1\n",
    "        elif ('dens' in T.name):\n",
    "            gradfcW, gradx = grad_fully_connected(W=VS[vs],R=VS[vs+1],back_propped=gradx,below=pre)\n",
    "            assign_op_fcW = update_only_non_zero(VS[vs],gradfcW,step_size)\n",
    "            #assign_op_fcW=tf.assign(VS[vs],VS[vs]-step_size*gradfcW)\n",
    "            OPLIST.append(assign_op_fcW)\n",
    "            if (len(VS[vs+1].shape.as_list())==2):\n",
    "                assign_op_fcR = update_only_non_zero(VS[vs+1],gradfcW,Rstep_size)\n",
    "                #assign_op_fcR=tf.assign(VS[vs+1],VS[vs+1]-Rstep_size*gradfcW)\n",
    "                OPLIST.append(assign_op_fcR)\n",
    "            ts+=1\n",
    "            vs+=2\n",
    "        if (T.name in sibs):\n",
    "            grad_hold=gradx\n",
    "            parent=sibs[T.name]\n",
    "            grad_hold_var[parent]=grad_hold\n",
    "\n",
    "\n",
    "    #print('Length of VS',len(VS),'Length of OPLIST',len(OPLIST))\n",
    "    OPLIST.append(acc)\n",
    "    OPLIST.append(cross_entropy)\n",
    "    \n",
    "    return OPLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def one_hot(values,n_values=10):\n",
    "    n_v = np.maximum(n_values,np.max(values) + 1)\n",
    "    oh=np.eye(n_v)[values]\n",
    "    return oh\n",
    "\n",
    "def get_cifar(data_set='cifar10'):\n",
    "    \n",
    "    filename = '/project2/cmsc25025/mnist/'+data_set+'_train.hdf5'\n",
    "    print(filename)\n",
    "    f = h5py.File(filename, 'r')\n",
    "    key = list(f.keys())[0]\n",
    "    # Get the data\n",
    "    tr = f[key]\n",
    "    print('tr',tr.shape)\n",
    "    key = list(f.keys())[1]\n",
    "    tr_lb=f[key]\n",
    "    train_data=np.float32(tr[0:45000])/255.\n",
    "    train_labels=one_hot(np.int32(tr_lb[0:45000]))\n",
    "    val_data=np.float32(tr[45000:])/255.\n",
    "    val_labels=one_hot(np.int32(tr_lb[45000:]))\n",
    "    filename = '/project2/cmsc25025/mnist/'+data_set+'_test.hdf5'\n",
    "    f = h5py.File(filename, 'r')\n",
    "    key = list(f.keys())[0]\n",
    "    # Get the data\n",
    "    test_data = np.float32(f[key])/255.\n",
    "    key = list(f.keys())[1]\n",
    "    test_labels=one_hot(np.int32(f[key]))\n",
    "    return (train_data, train_labels), (val_data, val_labels), (test_data, test_labels)\n",
    "\n",
    "def get_data(data_set):\n",
    "    if ('cifar' in data_set):\n",
    "        return(get_cifar(data_set=data_set))\n",
    "    elif (data_set==\"mnist\"):\n",
    "        return(get_mnist())\n",
    "    elif (data_set==\"mnist_transform\"):\n",
    "        return(get_mnist_trans())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get loss and accuracy from only one run of the feature extraction network\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def get_stats(data,labels,fc):\n",
    "    t1=time.time()\n",
    "    lo=0.\n",
    "    acc=0.\n",
    "    delta=batch_size\n",
    "    rr=np.arange(0,data.shape[0],delta)\n",
    "    for i in rr:\n",
    "        fc_out=fc.eval(feed_dict={x: data[i:i+delta], y_:labels[i:i+delta]})\n",
    "        log_sf=logsumexp(fc_out,axis=1).reshape((fc_out.shape[0],1))-fc_out\n",
    "        lo+=np.mean(np.sum(labels[i:i+delta]*log_sf, axis=1))\n",
    "        acc += np.mean(np.equal(np.argmax(fc_out, axis=1),np.argmax(labels[i:i+delta], axis=1)))\n",
    "    acc=acc/np.float32(len(rr))\n",
    "    lo=lo/np.float32(len(rr))\n",
    "    print('get stats time',time.time()-t1)\n",
    "    # We return the final functions (they contain all the information about the graph of the network)\n",
    "    return lo, acc\n",
    "\n",
    "# Run the iterations of one epoch\n",
    "def run_epoch(train,val,ii):\n",
    "        t1=time.time()\n",
    "        # Randomly shuffle the training data\n",
    "        np.random.shuffle(ii)\n",
    "        tr=train[0][ii]\n",
    "        y=train[1][ii]\n",
    "        lo=0.\n",
    "        acc=0.\n",
    "        ca=0.\n",
    "        #VS=tf.trainable_variables()\n",
    "        # Run disjoint batches on shuffled data\n",
    "        for j in np.arange(0,len(y),batch_size):\n",
    "            #if (np.mod(j,5000)==0):\n",
    "            #    print('Batch',j/batch_size)\n",
    "            batch=(tr[j:j+batch_size],y[j:j+batch_size])\n",
    "            grad=sess.run(dW_OPs,feed_dict={x: batch[0], y_: batch[1]})\n",
    "            \n",
    "            acc+=grad[-2]\n",
    "            lo+=grad[-1]\n",
    "            ca+=1\n",
    "        print('Epoch time',time.time()-t1)\n",
    "        return acc/ca, lo/ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:45239\n",
      "num_epochs:200\n",
      "data_set:cifar100\n",
      "batch_size:500\n",
      "eta_init:.1\n",
      "num_train:50000\n",
      "#eta_schedule:(100.,.01,120,.001)\n",
      "dep_fac:1.\n",
      "hinge:1.\n",
      "force_global_prob:(1.,-1.)\n",
      "name:input1\n",
      "name:conv0;parent:input1;filter_size:(3,3);num_filters:16;stride:(1,1)\n",
      "name:conv0aR;parent:conv0;stride:(1,1);filter_size:(3,3);num_filters:16\n",
      "name:concatsum0;parent:[conv0,conv0aR]\n",
      "name:conv1R;parent:concatsum0;filter_size:(3,3);num_filters:32;stride:(1,1)\n",
      "name:conv1aR;parent:conv1R;stride:(1,1);filter_size:(3,3);num_filters:32\n",
      "name:concatsum1;parent:[conv1R,conv1aR]\n",
      "name:pool1;parent:concatsum1;pool_size:(2,2);stride:(2,2);mode:max\n",
      "name:drop1;drop:.5;parent:pool1\n",
      "name:conv2R;parent:drop1;filter_size:(3,3);num_filters:64;stride:(1,1)\n",
      "name:conv2aR;parent:conv2R;stride:(1,1);filter_size:(3,3);num_filters:64\n",
      "name:concatsum2;parent:[conv2R,conv2aR]\n",
      "name:pool2;parent:concatsum2;pool_size:(2,2);stride:(2,2)\n",
      "name:conv3R;parent:pool2;filter_size:(3,3);num_filters:128;stride:(1,1)\n",
      "name:conv3aR;parent:conv3R;stride:(1,1);filter_size:(3,3);num_filters:128\n",
      "name:concatsum3;parent:[conv3R,conv3aR]\n",
      "name:pool3;parent:concatsum3;pool_size:(2,2);stride:(2,2);mode:max\n",
      "name:drop2;drop:.8;parent:pool3\n",
      "name:newdensp;parent:drop2;num_units:500\n",
      "name:newdensf;parent:newdensp;num_units:10;final:finalRstep_size -0.1\n",
      "/project2/cmsc25025/mnist/cifar100_train.hdf5\n",
      "tr (50000, 32, 32, 3)\n",
      "n_classes 100 dim 32 nchannels 3\n",
      "WARNING:tensorflow:From <ipython-input-5-7d41ba9ccf24>:69: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "sibs {'concatsum0/Add:0': 'conv0', 'concatsum1/Add:0': 'conv1R', 'concatsum2/Add:0': 'conv2R', 'concatsum3/Add:0': 'conv3R'}\n",
      "Tensor(\"newdensf/MatMul:0\", shape=(500, 100), dtype=float32)\n",
      "Tensor(\"newdensp/MatMul:0\", shape=(500, 500), dtype=float32)\n",
      "Tensor(\"drop2/Select:0\", shape=(500, 4, 4, 128), dtype=float32)\n",
      "Tensor(\"pool3/Equal:0\", shape=(500, 8, 8, 128), dtype=bool)\n",
      "Tensor(\"pool3/MaxPool:0\", shape=(500, 4, 4, 128), dtype=float32)\n",
      "Tensor(\"concatsum3/Add:0\", shape=(500, 8, 8, 128), dtype=float32)\n",
      "Tensor(\"conv3aR/clip_by_value:0\", shape=(500, 8, 8, 128), dtype=float32)\n",
      "Tensor(\"conv3R/clip_by_value:0\", shape=(500, 8, 8, 128), dtype=float32)\n",
      "Tensor(\"pool2/Equal:0\", shape=(500, 16, 16, 64), dtype=bool)\n",
      "Tensor(\"pool2/MaxPool:0\", shape=(500, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"concatsum2/Add:0\", shape=(500, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"conv2aR/clip_by_value:0\", shape=(500, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"conv2R/clip_by_value:0\", shape=(500, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"drop1/Select:0\", shape=(500, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"pool1/Equal:0\", shape=(?, 32, 32, 32), dtype=bool)\n",
      "Tensor(\"pool1/MaxPool:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"concatsum1/Add:0\", shape=(?, 32, 32, 32), dtype=float32)\n",
      "Tensor(\"conv1aR/clip_by_value:0\", shape=(?, 32, 32, 32), dtype=float32)\n",
      "Tensor(\"conv1R/clip_by_value:0\", shape=(?, 32, 32, 32), dtype=float32)\n",
      "Tensor(\"concatsum0/Add:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"conv0aR/clip_by_value:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"conv0/clip_by_value:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "newdensf/W:0 [500, 100] 0.05779106\n",
      "newdensf/R:0 [1] 0.0\n",
      "newdensp/W:0 [2048, 500] 0.028034022\n",
      "newdensp/R:0 [1] 0.0\n",
      "conv3aR/W:0 [3, 3, 128, 128] 0.02951888\n",
      "conv3aR/R:0 [1, 1] 0.0\n",
      "conv3R/W:0 [3, 3, 64, 128] 0.033996753\n",
      "conv3R/R:0 [1, 1] 0.0\n",
      "conv2aR/W:0 [3, 3, 64, 64] 0.041726872\n",
      "conv2aR/R:0 [1, 1] 0.0\n",
      "conv2R/W:0 [3, 3, 32, 64] 0.04796222\n",
      "conv2R/R:0 [1, 1] 0.0\n",
      "conv1aR/W:0 [3, 3, 32, 32] 0.058696322\n",
      "conv1aR/R:0 [1, 1] 0.0\n",
      "conv1R/W:0 [3, 3, 16, 32] 0.06877368\n",
      "conv1R/R:0 [1, 1] 0.0\n",
      "conv0aR/W:0 [3, 3, 16, 16] 0.084294185\n",
      "conv0aR/R:0 [1, 1] 0.0\n",
      "conv0/W:0 [3, 3, 3, 16] 0.11223246\n",
      "conv0/R:0 [1, 1] 0.0\n",
      "t 0 zeros 0 1.1708772\n",
      "t 2 zeros 0 0.5738779\n",
      "t 4 zeros 0 0.23526496\n",
      "t 6 zeros 0 1.0167155\n",
      "t 8 zeros 0 1.0903833\n",
      "t 10 zeros 0 1.5828003\n",
      "t 12 zeros 0 1.3854173\n",
      "t 14 zeros 0 1.3265177\n",
      "t 16 zeros 0 1.0049446\n",
      "t 18 zeros 0 0.5141293\n",
      "grad_hold Tensor(\"Select_2:0\", shape=(500, 8, 8, 128), dtype=float32)\n",
      "grad_hold Tensor(\"Select_7:0\", shape=(500, 16, 16, 64), dtype=float32)\n",
      "grad_hold Tensor(\"Select_12:0\", shape=(500, 32, 32, 32), dtype=float32)\n",
      "grad_hold Tensor(\"Conv2DBackpropInput_5:0\", shape=(500, 32, 32, 16), dtype=float32)\n",
      "Epoch time 21.338525533676147\n",
      "Epoch 0 Train loss, accuracy 4.727415853076511 0.015644444388130473\n",
      "get stats time 0.5229153633117676\n",
      "EPoch 0 Validation loss, accuracy 4.799842337846757 0.016800000000000002\n",
      "Epoch time 12.732553720474243\n",
      "Epoch 1 Train loss, accuracy 4.5581540849473745 0.03606666661798954\n",
      "get stats time 0.4391002655029297\n",
      "EPoch 1 Validation loss, accuracy 4.3474811191082 0.055600000000000004\n",
      "Epoch time 12.733001947402954\n",
      "Epoch 2 Train loss, accuracy 4.306612491607666 0.06015555577145682\n",
      "get stats time 0.43921661376953125\n",
      "EPoch 2 Validation loss, accuracy 4.198144633483887 0.0766\n",
      "Epoch time 12.742428302764893\n",
      "Epoch 3 Train loss, accuracy 4.137295365333557 0.08031111152635681\n",
      "get stats time 0.4304633140563965\n",
      "EPoch 3 Validation loss, accuracy 4.090531718349456 0.0948\n",
      "Epoch time 12.605846643447876\n",
      "Epoch 4 Train loss, accuracy 3.9990520583258733 0.09777777811719311\n",
      "get stats time 0.43906521797180176\n",
      "EPoch 4 Validation loss, accuracy 4.058732622098923 0.09599999999999999\n",
      "Epoch time 12.712088346481323\n",
      "Epoch 5 Train loss, accuracy 3.850441400210063 0.11835555583238602\n",
      "get stats time 0.43215203285217285\n",
      "EPoch 5 Validation loss, accuracy 3.782447902965546 0.12980000000000003\n",
      "Epoch time 12.626245021820068\n",
      "Epoch 6 Train loss, accuracy 3.7664687236150107 0.1295777781142129\n",
      "get stats time 0.4317135810852051\n",
      "EPoch 6 Validation loss, accuracy 3.839034563922882 0.1242\n",
      "Epoch time 12.625688314437866\n",
      "Epoch 7 Train loss, accuracy 3.6206674602296616 0.15264444450537365\n",
      "get stats time 0.43840742111206055\n",
      "EPoch 7 Validation loss, accuracy 4.241236429309845 0.0796\n",
      "Epoch time 12.723860025405884\n",
      "Epoch 8 Train loss, accuracy 3.5697849565082125 0.1629555554025703\n",
      "get stats time 0.4319033622741699\n",
      "EPoch 8 Validation loss, accuracy 3.5104053159713744 0.16879999999999998\n",
      "Epoch time 12.6552734375\n",
      "Epoch 9 Train loss, accuracy 3.4463604953553943 0.18344444500075446\n",
      "get stats time 0.43123793601989746\n",
      "EPoch 9 Validation loss, accuracy 4.574138268280029 0.06280000000000001\n",
      "Epoch time 12.626432657241821\n",
      "Epoch 10 Train loss, accuracy 3.5310317171944514 0.17397777769300674\n",
      "get stats time 0.4402294158935547\n",
      "EPoch 10 Validation loss, accuracy 3.4499736932277685 0.19139999999999996\n",
      "Epoch time 12.722240447998047\n",
      "Epoch 11 Train loss, accuracy 3.3078589068518744 0.20751111143165166\n",
      "get stats time 0.4313647747039795\n",
      "EPoch 11 Validation loss, accuracy 3.437559710264206 0.19039999999999996\n",
      "Epoch time 12.622179746627808\n",
      "Epoch 12 Train loss, accuracy 3.26158271100786 0.21753333359956742\n",
      "get stats time 0.43264198303222656\n",
      "EPoch 12 Validation loss, accuracy 3.30206552157402 0.20659999999999998\n",
      "Epoch time 12.639256715774536\n",
      "Epoch 13 Train loss, accuracy 3.1769463804033067 0.23242222219705583\n",
      "get stats time 0.4333195686340332\n",
      "EPoch 13 Validation loss, accuracy 3.3574140574455265 0.2022\n",
      "Epoch time 12.716780662536621\n",
      "Epoch 14 Train loss, accuracy 3.1251495864656236 0.24106666776869032\n",
      "get stats time 0.438798189163208\n",
      "EPoch 14 Validation loss, accuracy 3.160038143968582 0.23399999999999999\n",
      "Epoch time 12.63271713256836\n",
      "Epoch 15 Train loss, accuracy 3.0758039156595864 0.2510444439119763\n",
      "get stats time 0.4315779209136963\n",
      "EPoch 15 Validation loss, accuracy 3.2057077396392826 0.2326\n",
      "Epoch time 12.672870397567749\n",
      "Epoch 16 Train loss, accuracy 3.1193546772003176 0.2480000001274877\n",
      "get stats time 0.43201303482055664\n",
      "EPoch 16 Validation loss, accuracy 3.2860362321853636 0.22079999999999997\n",
      "Epoch time 12.622506618499756\n",
      "Epoch 17 Train loss, accuracy 2.9923401249779595 0.26766666571299236\n",
      "get stats time 0.4331319332122803\n",
      "EPoch 17 Validation loss, accuracy 3.2016826172828674 0.23200000000000004\n",
      "Epoch time 12.749537944793701\n",
      "Epoch 18 Train loss, accuracy 2.9902033885320027 0.2674000004927317\n",
      "get stats time 0.4393441677093506\n",
      "EPoch 18 Validation loss, accuracy 3.128897197198868 0.24359999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time 12.640021562576294\n",
      "Epoch 19 Train loss, accuracy 2.916568634245131 0.28051111300786336\n",
      "get stats time 0.4323275089263916\n",
      "EPoch 19 Validation loss, accuracy 3.2410026004791264 0.22400000000000003\n",
      "Epoch time 12.671654224395752\n",
      "Epoch 20 Train loss, accuracy 2.9070251756244234 0.2831999998953607\n",
      "get stats time 0.4328954219818115\n",
      "EPoch 20 Validation loss, accuracy 3.1070886329650884 0.2524\n",
      "Epoch time 12.621681690216064\n",
      "Epoch 21 Train loss, accuracy 2.885306543774075 0.2897777787513203\n",
      "get stats time 0.4330124855041504\n",
      "EPoch 21 Validation loss, accuracy 3.03529112739563 0.2598\n",
      "Epoch time 12.691030263900757\n",
      "Epoch 22 Train loss, accuracy 2.8184059884813095 0.29846666521496246\n",
      "get stats time 0.44062018394470215\n",
      "EPoch 22 Validation loss, accuracy 3.073011648416519 0.25880000000000003\n",
      "Epoch time 12.676113605499268\n",
      "Epoch 23 Train loss, accuracy 2.8090037769741483 0.3020000014040205\n",
      "get stats time 0.43386292457580566\n",
      "EPoch 23 Validation loss, accuracy 3.0034106044769286 0.2664\n",
      "Epoch time 12.609131097793579\n",
      "Epoch 24 Train loss, accuracy 2.76486939324273 0.3097111137376891\n",
      "get stats time 0.432081937789917\n",
      "EPoch 24 Validation loss, accuracy 2.979000690841675 0.27360000000000007\n",
      "Epoch time 12.671982765197754\n",
      "Epoch 25 Train loss, accuracy 2.757608021630181 0.3099333332644569\n",
      "get stats time 0.4387388229370117\n",
      "EPoch 25 Validation loss, accuracy 2.8534699231147767 0.2932\n",
      "Epoch time 12.655694484710693\n",
      "Epoch 26 Train loss, accuracy 2.732124866379632 0.3161555548508962\n",
      "get stats time 0.4327244758605957\n",
      "EPoch 26 Validation loss, accuracy 3.0103108790397646 0.27280000000000004\n",
      "Epoch time 12.599581718444824\n",
      "Epoch 27 Train loss, accuracy 2.7401911788516573 0.31526666730642317\n",
      "get stats time 0.43167829513549805\n",
      "EPoch 27 Validation loss, accuracy 2.939802681159973 0.27280000000000004\n",
      "Epoch time 12.608337879180908\n",
      "Epoch 28 Train loss, accuracy 2.693753621313307 0.32444444431198965\n",
      "get stats time 0.43102025985717773\n",
      "EPoch 28 Validation loss, accuracy 2.9615939627647405 0.2796\n",
      "Epoch time 12.601610660552979\n",
      "Epoch 29 Train loss, accuracy 2.6515258073806764 0.3340444452232785\n",
      "get stats time 0.43215274810791016\n",
      "EPoch 29 Validation loss, accuracy 2.8699208795547486 0.3034\n",
      "Epoch time 12.693692922592163\n",
      "Epoch 30 Train loss, accuracy 2.6456250747044883 0.3355333334869809\n",
      "get stats time 0.43755030632019043\n",
      "EPoch 30 Validation loss, accuracy 2.918490876674652 0.2882\n",
      "Epoch time 12.599052667617798\n",
      "Epoch 31 Train loss, accuracy 2.6275887648264566 0.3344444430536694\n",
      "get stats time 0.43116235733032227\n",
      "EPoch 31 Validation loss, accuracy 2.8531891067028043 0.30060000000000003\n",
      "Epoch time 12.603048086166382\n",
      "Epoch 32 Train loss, accuracy 2.5929461161295575 0.34395555456479393\n",
      "get stats time 0.43198084831237793\n",
      "EPoch 32 Validation loss, accuracy 2.7726461398601536 0.31679999999999997\n",
      "Epoch time 12.610583305358887\n",
      "Epoch 33 Train loss, accuracy 2.5998754024505617 0.34162222113874224\n",
      "get stats time 0.43364548683166504\n",
      "EPoch 33 Validation loss, accuracy 2.7890051409244543 0.31279999999999997\n",
      "Epoch time 12.708294868469238\n",
      "Epoch 34 Train loss, accuracy 2.561887179480659 0.35044444534513686\n",
      "get stats time 0.4385404586791992\n",
      "EPoch 34 Validation loss, accuracy 2.880033346605301 0.2926\n",
      "Epoch time 12.616897106170654\n",
      "Epoch 35 Train loss, accuracy 2.5783923546473186 0.3472222218910853\n",
      "get stats time 0.43213319778442383\n",
      "EPoch 35 Validation loss, accuracy 2.803713626432419 0.3088\n",
      "Epoch time 12.649789333343506\n",
      "Epoch 36 Train loss, accuracy 2.5445693996217518 0.3534888896677229\n",
      "get stats time 0.432528018951416\n",
      "EPoch 36 Validation loss, accuracy 2.750302295303345 0.3218\n",
      "Epoch time 12.617250680923462\n",
      "Epoch 37 Train loss, accuracy 2.5251440313127307 0.3574222236871719\n",
      "get stats time 0.43419694900512695\n",
      "EPoch 37 Validation loss, accuracy 2.7910193347930905 0.3102\n",
      "Epoch time 12.726292848587036\n",
      "Epoch 38 Train loss, accuracy 2.504654145240784 0.35806666645738816\n",
      "get stats time 0.43921399116516113\n",
      "EPoch 38 Validation loss, accuracy 2.7346724071025847 0.3212\n",
      "Epoch time 12.622079849243164\n",
      "Epoch 39 Train loss, accuracy 2.4882022274865045 0.3659333328406016\n",
      "get stats time 0.4312021732330322\n",
      "EPoch 39 Validation loss, accuracy 2.802252242231369 0.321\n",
      "Epoch time 12.661429643630981\n",
      "Epoch 40 Train loss, accuracy 2.500722132788764 0.3617777794599533\n",
      "get stats time 0.4337937831878662\n",
      "EPoch 40 Validation loss, accuracy 2.783377252483368 0.30779999999999996\n",
      "Epoch time 12.620023012161255\n",
      "Epoch 41 Train loss, accuracy 2.510820926560296 0.36188888880941605\n",
      "get stats time 0.4339754581451416\n",
      "EPoch 41 Validation loss, accuracy 2.756720654296875 0.31900000000000006\n",
      "Epoch time 12.675549983978271\n",
      "Epoch 42 Train loss, accuracy 2.483143515057034 0.364088887307379\n",
      "get stats time 0.4401857852935791\n",
      "EPoch 42 Validation loss, accuracy 2.726237740087509 0.3238\n",
      "Epoch time 12.677074193954468\n",
      "Epoch 43 Train loss, accuracy 2.479590106010437 0.3668888903326458\n",
      "get stats time 0.43332624435424805\n",
      "EPoch 43 Validation loss, accuracy 2.793585725069046 0.3184\n",
      "Epoch time 12.644108533859253\n",
      "Epoch 44 Train loss, accuracy 2.4462804238001508 0.37260000043445163\n",
      "get stats time 0.43037986755371094\n",
      "EPoch 44 Validation loss, accuracy 2.7084525257587435 0.31720000000000004\n",
      "Epoch time 12.604186534881592\n",
      "Epoch 45 Train loss, accuracy 2.4982159402635364 0.3674666672945023\n",
      "get stats time 0.43194079399108887\n",
      "EPoch 45 Validation loss, accuracy 2.686061812067032 0.3426\n",
      "Epoch time 12.6708664894104\n",
      "Epoch 46 Train loss, accuracy 2.4297930081685384 0.3774222215016683\n",
      "get stats time 0.44002246856689453\n",
      "EPoch 46 Validation loss, accuracy 2.7340593913555145 0.32099999999999995\n",
      "Epoch time 12.658721446990967\n",
      "Epoch 47 Train loss, accuracy 2.432803930176629 0.3783333328035143\n",
      "get stats time 0.433056116104126\n",
      "EPoch 47 Validation loss, accuracy 2.656085544586182 0.33959999999999996\n",
      "Epoch time 12.600636959075928\n",
      "Epoch 48 Train loss, accuracy 2.4239174074596828 0.3785555554760827\n",
      "get stats time 0.4323868751525879\n",
      "EPoch 48 Validation loss, accuracy 2.7323686867713928 0.3268\n",
      "Epoch time 12.60781455039978\n",
      "Epoch 49 Train loss, accuracy 2.4054700983895194 0.38184444473849405\n",
      "get stats time 0.43120479583740234\n",
      "EPoch 49 Validation loss, accuracy 2.6589272227764127 0.3404\n",
      "Epoch time 12.643837690353394\n",
      "Epoch 53 Train loss, accuracy 2.374196113480462 0.3890444447596868\n",
      "get stats time 0.4320521354675293\n",
      "EPoch 53 Validation loss, accuracy 2.6475821291923523 0.34059999999999996\n",
      "Epoch time 12.616436243057251\n",
      "Epoch 54 Train loss, accuracy 2.366660311486986 0.3908888902929094\n",
      "get stats time 0.4335477352142334\n",
      "EPoch 54 Validation loss, accuracy 2.6296768223762514 0.3356\n",
      "Epoch time 12.675236940383911\n",
      "Epoch 55 Train loss, accuracy 2.4493043581644693 0.37986666328377194\n",
      "get stats time 0.43355512619018555\n",
      "EPoch 55 Validation loss, accuracy 2.6995814769744872 0.3318\n",
      "Epoch time 12.652839660644531\n",
      "Epoch 56 Train loss, accuracy 2.3506090879440307 0.3935111128621631\n",
      "get stats time 0.4387803077697754\n",
      "EPoch 56 Validation loss, accuracy 2.842542701864242 0.31379999999999997\n",
      "Epoch time 12.620652198791504\n",
      "Epoch 57 Train loss, accuracy 2.346999828020732 0.39211111068725585\n",
      "get stats time 0.43380308151245117\n",
      "EPoch 57 Validation loss, accuracy 2.680235911083221 0.33519999999999994\n",
      "Epoch time 12.618949890136719\n",
      "Epoch 58 Train loss, accuracy 2.338990518781874 0.3966444436046812\n",
      "get stats time 0.4317023754119873\n",
      "EPoch 58 Validation loss, accuracy 2.6003245996475224 0.35\n",
      "Epoch time 12.669759035110474\n",
      "Epoch 59 Train loss, accuracy 2.3189450184504192 0.400777777367168\n",
      "get stats time 0.4398925304412842\n",
      "EPoch 59 Validation loss, accuracy 2.601728225755692 0.35479999999999995\n",
      "Epoch time 12.655915975570679\n",
      "Epoch 60 Train loss, accuracy 2.3468121422661676 0.39433333343929716\n",
      "get stats time 0.43260931968688965\n",
      "EPoch 60 Validation loss, accuracy 2.983044727516174 0.2824\n",
      "Epoch time 12.598352193832397\n",
      "Epoch 61 Train loss, accuracy 2.3667175107532077 0.38948888977368673\n",
      "get stats time 0.4319322109222412\n",
      "EPoch 61 Validation loss, accuracy 2.6112872693538667 0.34619999999999995\n",
      "Epoch time 12.580587387084961\n",
      "Epoch 62 Train loss, accuracy 2.3009557008743284 0.402466666036182\n",
      "get stats time 0.4315769672393799\n",
      "EPoch 62 Validation loss, accuracy 2.58232129907608 0.34879999999999994\n",
      "Epoch time 12.584370613098145\n",
      "Epoch 63 Train loss, accuracy 2.3125623040729097 0.4015777766704559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get stats time 0.43196749687194824\n",
      "EPoch 63 Validation loss, accuracy 2.617912422609329 0.3442\n",
      "Epoch time 12.662503719329834\n",
      "Epoch 64 Train loss, accuracy 2.300118629137675 0.4036888877550761\n",
      "get stats time 0.437455415725708\n",
      "EPoch 64 Validation loss, accuracy 2.7797290401935575 0.324\n",
      "Epoch time 12.552242517471313\n",
      "Epoch 65 Train loss, accuracy 2.2812849097781713 0.4080666661262512\n",
      "get stats time 0.4306468963623047\n",
      "EPoch 65 Validation loss, accuracy 2.664098189163208 0.3378\n",
      "Epoch time 12.57861852645874\n",
      "Epoch 66 Train loss, accuracy 2.2892888095643786 0.4049777779314253\n",
      "get stats time 0.4300706386566162\n",
      "EPoch 66 Validation loss, accuracy 2.5935981766700746 0.351\n",
      "Epoch time 12.581362962722778\n",
      "Epoch 67 Train loss, accuracy 2.2784031920962864 0.40433333151870304\n",
      "get stats time 0.43331170082092285\n",
      "EPoch 67 Validation loss, accuracy 2.5702385149478912 0.3574\n",
      "Epoch time 12.693990230560303\n",
      "Epoch 68 Train loss, accuracy 2.2821395635604858 0.4091111093759537\n",
      "get stats time 0.43766355514526367\n",
      "EPoch 68 Validation loss, accuracy 2.585147402667999 0.3516\n",
      "Epoch time 12.593786478042603\n",
      "Epoch 69 Train loss, accuracy 2.2675621509552 0.4103555570046107\n",
      "get stats time 0.4318375587463379\n",
      "EPoch 69 Validation loss, accuracy 2.6229533864498142 0.33999999999999997\n",
      "Epoch time 12.622760772705078\n",
      "Epoch 70 Train loss, accuracy 2.264303374290466 0.40959999958674115\n",
      "get stats time 0.4324197769165039\n",
      "EPoch 70 Validation loss, accuracy 2.5752410269260406 0.34919999999999995\n",
      "Epoch time 12.590224981307983\n",
      "Epoch 71 Train loss, accuracy 2.2507552822430927 0.41486666599909466\n",
      "get stats time 0.43247175216674805\n",
      "EPoch 71 Validation loss, accuracy 2.549955860614777 0.35779999999999995\n",
      "Epoch time 12.701067209243774\n",
      "Epoch 72 Train loss, accuracy 2.246705730756124 0.41548888948228624\n",
      "get stats time 0.44988036155700684\n",
      "EPoch 72 Validation loss, accuracy 2.579877500343323 0.35179999999999995\n",
      "Epoch time 12.617615699768066\n",
      "Epoch 73 Train loss, accuracy 2.2548374467425876 0.4109111100435257\n",
      "get stats time 0.4318842887878418\n",
      "EPoch 73 Validation loss, accuracy 2.5649995028972623 0.35119999999999996\n",
      "Epoch time 12.631298303604126\n",
      "Epoch 74 Train loss, accuracy 2.25375365946028 0.4133111122581694\n",
      "get stats time 0.43204331398010254\n",
      "EPoch 74 Validation loss, accuracy 2.6324196350097653 0.3404\n",
      "Epoch time 12.603460311889648\n",
      "Epoch 75 Train loss, accuracy 2.232295560836792 0.41713333427906035\n",
      "get stats time 0.43216753005981445\n",
      "EPoch 75 Validation loss, accuracy 2.5209486382484436 0.3714\n",
      "Epoch time 12.672116994857788\n",
      "Epoch 76 Train loss, accuracy 2.2457798057132297 0.41197777754730647\n",
      "get stats time 0.44051599502563477\n",
      "EPoch 76 Validation loss, accuracy 2.5751333129405976 0.3552\n",
      "Epoch time 12.664313316345215\n",
      "Epoch 77 Train loss, accuracy 2.231221948729621 0.4174222230911255\n",
      "get stats time 0.43235039710998535\n",
      "EPoch 77 Validation loss, accuracy 2.538847677612305 0.3552\n",
      "Epoch time 12.642293214797974\n",
      "Epoch 78 Train loss, accuracy 2.2154093080096775 0.4214222229189343\n",
      "get stats time 0.4311244487762451\n",
      "EPoch 78 Validation loss, accuracy 2.578223318052292 0.3631999999999999\n",
      "Epoch time 12.611666440963745\n",
      "Epoch 79 Train loss, accuracy 2.223512726359897 0.420866667230924\n",
      "get stats time 0.4316885471343994\n",
      "EPoch 79 Validation loss, accuracy 2.554864622831345 0.3605999999999999\n",
      "Epoch time 12.668367147445679\n",
      "Epoch 80 Train loss, accuracy 2.230595503913032 0.41771111124091675\n",
      "get stats time 0.43889856338500977\n",
      "EPoch 80 Validation loss, accuracy 2.5672624200344085 0.357\n",
      "Epoch time 12.65430998802185\n",
      "Epoch 81 Train loss, accuracy 2.231215433279673 0.41544444296095107\n",
      "get stats time 0.432847261428833\n",
      "EPoch 81 Validation loss, accuracy 2.562851076793671 0.35100000000000003\n",
      "Epoch time 12.59656810760498\n",
      "Epoch 82 Train loss, accuracy 2.209443573156993 0.4208222223652734\n",
      "get stats time 0.43398046493530273\n",
      "EPoch 82 Validation loss, accuracy 2.595978925895691 0.35179999999999995\n",
      "Epoch time 12.571978569030762\n",
      "Epoch 83 Train loss, accuracy 2.1958398713005916 0.42086666491296554\n",
      "get stats time 0.4296436309814453\n",
      "EPoch 83 Validation loss, accuracy 2.6827854918956757 0.3448\n",
      "Epoch time 12.483171939849854\n",
      "Epoch 84 Train loss, accuracy 2.258801409933302 0.4103999998834398\n",
      "get stats time 0.4306182861328125\n",
      "EPoch 84 Validation loss, accuracy 2.5651254289627077 0.3574\n",
      "Epoch time 12.57062029838562\n",
      "Epoch 85 Train loss, accuracy 2.199791599644555 0.4220666663514243\n",
      "get stats time 0.43650054931640625\n",
      "EPoch 85 Validation loss, accuracy 2.526768730926514 0.36239999999999994\n",
      "Epoch time 12.475328922271729\n",
      "Epoch 86 Train loss, accuracy 2.2073089480400085 0.4223333345519172\n",
      "get stats time 0.42894816398620605\n",
      "EPoch 86 Validation loss, accuracy 2.5336705236911774 0.3716\n",
      "Epoch time 12.48215103149414\n",
      "Epoch 87 Train loss, accuracy 2.197395964463552 0.42215555475817784\n",
      "get stats time 0.42918848991394043\n",
      "EPoch 87 Validation loss, accuracy 2.6402073900222782 0.3432\n",
      "Epoch time 12.485511302947998\n",
      "Epoch 88 Train loss, accuracy 2.201623717943827 0.42562222315205467\n",
      "get stats time 0.43064260482788086\n",
      "EPoch 88 Validation loss, accuracy 2.5874064385890962 0.35459999999999997\n",
      "Epoch time 12.563871383666992\n",
      "Epoch 89 Train loss, accuracy 2.1933246493339538 0.4257333328326543\n",
      "get stats time 0.43631744384765625\n",
      "EPoch 89 Validation loss, accuracy 2.578662484550476 0.3538\n",
      "Epoch time 12.49897027015686\n",
      "Epoch 90 Train loss, accuracy 2.1962063749631247 0.42737777796056536\n",
      "get stats time 0.43132781982421875\n",
      "EPoch 90 Validation loss, accuracy 2.5117486125946042 0.36219999999999997\n"
     ]
    }
   ],
   "source": [
    "def zero_out_weights():\n",
    "        if (PARS['force_global_prob'][1]>=0 and PARS['force_global_prob'][0]<1.):\n",
    "            shape=v.get_shape().as_list()\n",
    "            Z=tf.zeros(shape)\n",
    "            U=tf.random_uniform(shape)\n",
    "            zero_op=tf.assign(v,K.tf.where(U<PARS['force_global_prob'][0],v,Z))\n",
    "            sess.run(zero_op)\n",
    "\n",
    "# Run the training\n",
    "import parse_net_pars as pp\n",
    "import time\n",
    "PARS={}\n",
    "\n",
    "net='fncrc_deep_tryR_avg'\n",
    "pp.parse_text_file(net,PARS,lname='layers', dump=True)\n",
    "batch_size=PARS['batch_size']\n",
    "step_size=PARS['eta_init']\n",
    "num_epochs=PARS['num_epochs']\n",
    "num_train=PARS['num_train']\n",
    "data_set=PARS['data_set']\n",
    "Rstep_size=list(PARS['force_global_prob'])[1]*step_size\n",
    "print('Rstep_size',Rstep_size)\n",
    "\n",
    "model_name=\"model\"\n",
    "\n",
    "train,val,test=get_data(data_set=data_set)\n",
    "num_train=np.minimum(num_train,train[0].shape[0])\n",
    "dim=train[0].shape[1]\n",
    "nchannels=train[0].shape[3]\n",
    "n_classes=train[1].shape[1]\n",
    "print('n_classes',n_classes,'dim',dim,'nchannels',nchannels)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, dim, dim, nchannels],name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None,n_classes],name=\"y\")\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Create the network architecture with the above placeholdes as the inputs.\n",
    "    #cross_entropy, accuracy, conv, convK, fc, pool, mask =create_small_network()\n",
    "    cross_entropy, accuracy, TS, sibs =create_network(PARS) \n",
    "    TS.reverse()\n",
    "    for t in TS:\n",
    "        print(t)\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Show trainable variables\n",
    "    VS=tf.trainable_variables()\n",
    "    VS.reverse()\n",
    "    for v in VS:\n",
    "        print(v.name,v.get_shape().as_list(),np.std(v.eval()))\n",
    "        zero_out_weights()\n",
    "    # Differences between W and R\n",
    "    for t in np.arange(0,len(VS),2):\n",
    "       print('t',t,'zeros',np.sum(VS[t].eval()==0), np.max(np.abs(VS[t].eval()-VS[t+1].eval())))\n",
    "    dW_OPs=back_prop() \n",
    "    ii=np.arange(0,num_train,1) \n",
    "   \n",
    "    # Run epochs\n",
    "    AC=[]\n",
    "    VAC=[]\n",
    "    for i in range(num_epochs):  # number of epochs\n",
    "        ac,lo=run_epoch(train,val,ii)\n",
    "        if (np.mod(i,1)==0):\n",
    "            #lo,ac = get_stats(train[0][0:num_train],train[1][0:num_train],TS[0])\n",
    "            AC.append(ac)\n",
    "            print('Epoch',i,'Train loss, accuracy',lo,ac)\n",
    "            vlo,vac = get_stats(val[0],val[1],TS[0])\n",
    "            VAC.append(vac)\n",
    "            print('EPoch',i,'Validation loss, accuracy',vlo,vac)\n",
    "            # Test set accuracy\n",
    "    AC=np.array(AC)\n",
    "    VAC=np.array(VAC)\n",
    "    lo,ac = get_stats(test[0],test[1],TS[0])\n",
    "    print('test accuracy %g' % ac)\n",
    "    plt.plot(AC)\n",
    "    plt.plot(VAC)\n",
    "    plt.show()\n",
    "    ACC=np.concatenate([np.expand_dims(AC,axis=1),np.expand_dims(VAC,axis=1)],axis=1)\n",
    "    np.save('ACC',ACC)\n",
    "    # Save model\n",
    "    #tf.add_to_collection(\"optimizer\", train_step)\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"tmp/\"+model_name)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL_GPU_cuda_9.0]",
   "language": "python",
   "name": "conda-env-DL_GPU_cuda_9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
