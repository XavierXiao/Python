{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32_ref> 1.0\n",
      "4.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    x=tf.Variable(1.)\n",
    "    x=x+x\n",
    "    y=x\n",
    "    x=2*x\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    VS=tf.trainable_variables()\n",
    "    for v in VS:\n",
    "        print(v,v.eval())\n",
    "    print(x.eval())\n",
    "    print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[4.]], dtype=float32)>\n",
      "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tfe = tf.contrib.eager\n",
    "w = tfe.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "print(loss)\n",
    "w.assign([[4.]])\n",
    "print(w)\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)  # => tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda2/envs/myenv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "tfe = tf.contrib.eager\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a1bc226d8>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=78, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=88, shape=(), dtype=float32, numpy=1.0>]\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(dy):\n",
    "    return dy * (1 - 1 / (1 + e))\n",
    "  return tf.log(1 + e), grad\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(0.))  # => [0.5]\n",
    "\n",
    "# And the gradient computation also works at x = 100.\n",
    "print(grad_log1pexp(100.))  # => [1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=96, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=104, shape=(), dtype=float32, numpy=nan>]\n"
     ]
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "  return tf.log(1 + tf.exp(x))\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# The gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(0.))  # => [0.5]\n",
    "\n",
    "# However, x = 100 fails because of numerical instability.\n",
    "print(grad_log1pexp(100.))  # => [nan]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.] [1. 4.] [10. 20.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "\n",
    "# Define custom py_func which takes also a grad op as argument:\n",
    "def py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n",
    "    \n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "    \n",
    "    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "    \n",
    "def ddd(x,y):\n",
    "    return(np.dot(x,y))\n",
    "    \n",
    "# Def custom square function using np.square instead of tf.square:\n",
    "def mydot(x, name=None):\n",
    "    \n",
    "    with ops.name_scope(name, \"Mysquare\", [x]) as name:\n",
    "        dot_xy = py_func(ddd,\n",
    "                        [x,y],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        grad=_MyDotGrad)  # <-- here's the call to the gradient\n",
    "        return sqr_x[0]\n",
    "\n",
    "# Actual gradient:\n",
    "def _MyDotGrad(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    return grad * 10 * x  # add a \"small\" error just to see the difference:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x = tf.constant([1., 2.])\n",
    "    y = mysquare(x)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    print(x.eval(), y.eval(), tf.gradients(y, x)[0].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[0.90305383 0.79923935 0.43836708 0.02828587 0.79660186 0.74410462]\n",
      " [0.53591689 0.70329835 0.87789272 0.06635842 0.71036043 0.96535111]\n",
      " [0.02301801 0.8350919  0.71760156 0.3623323  0.35450563 0.8573271 ]\n",
      " [0.04401364 0.53328624 0.43455272 0.53601437 0.43941282 0.7134037 ]\n",
      " [0.59063146 0.15909857 0.87981775 0.23979562 0.48883085 0.76140063]]\n",
      "c [[0.97263005 0.71296995 0.89880469 0.69892602 0.89331679 0.55664304]\n",
      " [0.85425185 0.68204215 0.02848011 0.70656992 0.67456518 0.3857768 ]\n",
      " [0.46465487 0.68939404 0.69556705 0.33168079 0.52856626 0.40907122]\n",
      " [0.68313617 0.7218877  0.0489139  0.30204473 0.00480236 0.87539432]\n",
      " [0.70129178 0.30340741 0.87525314 0.15383944 0.91328548 0.31331791]]\n",
      "b [[0.77112767]\n",
      " [0.86710884]\n",
      " [0.6446378 ]\n",
      " [0.50476649]\n",
      " [0.28629879]\n",
      " [0.24999904]]\n",
      "lossab 8.833874799734375\n",
      "losscb 9.778082294558654\n",
      "[[2.09663383]\n",
      " [3.03001441]\n",
      " [3.34823183]\n",
      " [1.23278659]\n",
      " [2.78971159]\n",
      " [4.04158717]]\n",
      "[[3.67596471]\n",
      " [3.10970126]\n",
      " [2.54701889]\n",
      " [2.19306089]\n",
      " [3.01453605]\n",
      " [2.54020329]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "a=tf.Variable(np.random.rand(5,6),\"a\")\n",
    "b=tf.Variable(np.random.rand(6,1),\"b\")\n",
    "c=tf.Variable(np.random.rand(5,6),\"c\")\n",
    "#b=tf.Variable(6.,\"a\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('a',a.eval())\n",
    "    print('c',c.eval())\n",
    "    print('b',b.eval())\n",
    "    lossab=tf.reduce_sum(tf.matmul(a,b))\n",
    "    losscb=tf.reduce_sum(tf.matmul(c,b))\n",
    "    print('lossab',lossab.eval())\n",
    "    print('losscb',losscb.eval())\n",
    "\n",
    "    gradab=tf.gradients(lossab,[b])\n",
    "    gradcb=tf.gradients(losscb,[b])\n",
    "    \n",
    "    print(gradab[0].eval())\n",
    "    print(gradcb[0].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1)\n",
    "b = tf.constant(2)\n",
    "c = a + b\n",
    "\n",
    "with tf.control_dependencies([c]):\n",
    "    assign = tf.assign(a, 5)\n",
    "\n",
    "sess = tf.Session()\n",
    "for i in range(10):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run([assign, c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
